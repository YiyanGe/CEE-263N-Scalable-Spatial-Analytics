{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3. Clustering: 1 million samples problem (40 points). This part deals with the full dataset of 1 million tweets. Your task is to design a system that can handle spatial clustering of 1M samples.\n",
    "Considering the memory limitations and scaling properties of the algorithms studied in Part 2, design the clustering system that can be applied to the full dataset. Consider using a hierarchical approach with two (or more) processing stages, where DBScan is applied to each cluster obtained from a run of mini-batch k-means. By varying the parameters of the algorithms, optimize the processing time required to detect clusters of tweets that correspond to important locations in California. We will consider a location “important” if it is characterized with a cluster’s core of at least 100 samples within a radius of 100 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io\n",
    "import math\n",
    "import json\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/tweets_1M.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5ab27634845f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/tweets_1M.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/tweets_1M.json'"
     ]
    }
   ],
   "source": [
    "with open('data/tweets_1M.json','r') as f:\n",
    "    tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[tweets[x]['lat'],tweets[x]['lng']] for x in range(0, len(tweets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frange(start, stop, step):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utm\n",
    "for n in range(0, len(X)):\n",
    "    meters = utm.from_latlon(X[n][0],X[n][1])\n",
    "    X[n][0] = meters[0]\n",
    "    X[n][1] = meters[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corresponds to step 3 in the report\n",
    "for perc in [0.002]:\n",
    "    \n",
    "    for n in range(64, 65):\n",
    "    \n",
    "        batch_size=int(len(X)*perc)\n",
    "        ttl_t = 0\n",
    "\n",
    "        mbk = MiniBatchKMeans(init='k-means++', n_clusters=n, batch_size=batch_size,\n",
    "                              n_init=10, max_no_improvement=10, verbose=0)\n",
    "        t0 = time.time()\n",
    "        mbk.fit(X)\n",
    "        t_mini_batch = time.time() - t0\n",
    "        ttl_t += t_mini_batch\n",
    "\n",
    "        mbk_means_labels = mbk.labels_\n",
    "        mbk_means_cluster_centers = mbk.cluster_centers_\n",
    "        mbk_means_labels_unique = np.unique(mbk_means_labels)\n",
    "\n",
    "        for cluster_label in list(mbk_means_labels_unique):\n",
    "\n",
    "            mask = (mbk_means_labels == cluster_label)\n",
    "            X_cluster = X[mask]\n",
    "\n",
    "            eps = 100\n",
    "            t_db = time.time()\n",
    "            db = DBSCAN(eps=eps, min_samples=100).fit(X_cluster)\n",
    "            t_fin_db = time.time() - t_db\n",
    "\n",
    "            ttl_t += t_fin_db\n",
    "\n",
    "        print (perc*1000000, n, ttl_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
